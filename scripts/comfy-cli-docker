#!/usr/bin/env bash
# License: GPLv3
# Credits: Felipe Facundes
# Source: https://github.com/comfyanonymous/ComfyUI
# Tutorial: https://www.youtube.com/watch?v=TLE9NmN_sxw
#           https://youtu.be/xWUddF5oMb0
# Docker version for Python 3.13

# Function to display help menu
show_help() {
    cat << EOF

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                  COMFY-CLI DOCKER SCRIPT (Python 3.13)           ‚ïë
‚ïë                     Containerized Installation                    ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

DESCRIPTION:
    This script creates and manages a Docker container for ComfyUI CLI
    with Python 3.13, automatic GPU detection, and persistent storage.

USAGE:
    $0 [COMMAND] [OPTIONS]

COMMANDS:
    run         Start ComfyUI CLI in Docker container (default if no command)
    build       Build Docker image with Python 3.13
    shell       Open interactive shell in container
    stop        Stop running container
    logs        View container logs
    clean       Remove container and images (keeps data volume)
    purge       Remove everything including data volume
    help        Show this help message

OPTIONS for 'run' command:
    -g, --gpu          Use GPU acceleration (NVIDIA/AMD)
    -p, --port PORT    Map container port to host (default: 8188)
    -n, --name NAME    Container name (default: comfy-cli)
    -v, --volume DIR   Mount host directory as volume
    -nu, --nvidia-universal  Force CUDA 12.6 for NVIDIA (compatibility mode)

EXAMPLES:
    # Run with GPU support and CUDA 12.6 (compatibility mode)
    $0 run --gpu -nu
    
    # Run with GPU detection (auto)
    $0 run --gpu
    
    # Run with custom port and volume
    $0 run --port 8080 --volume ./models:/app/models
    
    # Build the Docker image
    $0 build
    
    # Open shell in container
    $0 shell
    
    # View logs
    $0 logs

DOCKER IMAGE FEATURES:
    ‚Ä¢ Python 3.13 with latest pip
    ‚Ä¢ Automatic GPU detection and PyTorch installation
    ‚Ä¢ NVIDIA Universal mode (-nu) for CUDA 12.6 compatibility
    ‚Ä¢ Persistent data volume: comfy-cli-data
    ‚Ä¢ CUDA support for NVIDIA GPUs
    ‚Ä¢ ROCm support for AMD GPUs
    ‚Ä¢ XPU support for Intel GPUs
    ‚Ä¢ CPU fallback mode

PYTORCH INSTALLATION MODES:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Mode             ‚îÇ PyTorch Version                         ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ NVIDIA (auto)    ‚îÇ Based on driver version detection       ‚îÇ
    ‚îÇ NVIDIA (-nu)     ‚îÇ CUDA 12.6 (torch==2.9.0)                ‚îÇ
    ‚îÇ AMD              ‚îÇ ROCm 6.4                                ‚îÇ
    ‚îÇ Intel            ‚îÇ XPU                                     ‚îÇ
    ‚îÇ CPU              ‚îÇ CPU-only version                        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

VOLUME STRUCTURE:
    ‚Ä¢ /app/comfy-ui          - ComfyUI installation
    ‚Ä¢ /app/models            - AI models storage
    ‚Ä¢ /app/output            - Generated outputs
    ‚Ä¢ /app/config            - Configuration files

GPU SUPPORT IN DOCKER:
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ GPU Type         ‚îÇ Docker Runtime / Requirements       ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ NVIDIA           | --gpus all (requires nvidia-docker2)‚îÇ
    ‚îÇ AMD              | --device /dev/kfd --device /dev/dri ‚îÇ
    ‚îÇ Intel            | --device /dev/dri                   ‚îÇ
    ‚îÇ CPU              | No special flags needed             ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

PREREQUISITES:
    ‚Ä¢ Docker Engine 20.10+
    ‚Ä¢ For NVIDIA: nvidia-docker2 and NVIDIA drivers
    ‚Ä¢ For AMD: ROCm Docker support
    ‚Ä¢ 10GB+ free disk space

TROUBLESHOOTING:
    ‚Ä¢ Check Docker is running: docker info
    ‚Ä¢ Test GPU access: docker run --gpus all nvidia/cuda:13.0-base nvidia-smi
    ‚Ä¢ Check container status: docker ps -a
    ‚Ä¢ View detailed logs: $0 logs
    ‚Ä¢ Use -nu flag for NVIDIA compatibility issues

PERSISTENT DATA:
    Data is preserved in Docker volume 'comfy-cli-data'
    To backup: docker run --rm -v comfy-cli-data:/source -v \$(pwd):/backup \\
                alpine tar czf /backup/comfy-backup.tar.gz -C /source .

EOF
    exit 0
}

# Default configuration
CONTAINER_NAME="comfy-cli"
IMAGE_NAME="comfy-cli-python3.13"
DATA_VOLUME="comfy-cli-data"
HOST_PORT="8188"
CONTAINER_PORT="8188"
USE_GPU=false
HOST_VOLUME=""
FORCE_CUDA126=false

# Parse command line arguments
parse_arguments() {
    COMMAND="${1:-run}"
    
    case "$COMMAND" in
        run|build|shell|stop|logs|clean|purge|help)
            shift
            ;;
        *)
            COMMAND="run"
            ;;
    esac
    
    while [[ $# -gt 0 ]]; do
        case "$1" in
            -g|--gpu)
                USE_GPU=true
                shift
                ;;
            -nu|--nvidia-universal)
                FORCE_CUDA126=true
                shift
                ;;
            -p|--port)
                HOST_PORT="$2"
                shift 2
                ;;
            -n|--name)
                CONTAINER_NAME="$2"
                shift 2
                ;;
            -v|--volume)
                HOST_VOLUME="$2"
                shift 2
                ;;
            -h|--help)
                show_help
                ;;
            *)
                echo "‚ö†Ô∏è Unknown option: $1"
                show_help
                ;;
        esac
    done
}

# Build Docker image with Python 3.13
build_image() {
    echo "üöÄ Building Docker image with Python 3.13..."
    
    cat > Dockerfile << 'DOCKERFILE'
FROM python:3.13-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    wget \
    curl \
    git \
    libgl1-mesa-glx \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    libgomp1 \
    ocl-icd-libopencl1 \
    clinfo \
    pciutils \
    && rm -rf /var/lib/apt/lists/*

# Create directories for persistence
RUN mkdir -p /app/comfy-ui /app/models /app/output /app/config

# Function to detect and install appropriate PyTorch
COPY --chmod=755 << 'EOF' /app/detect_gpu.sh
#!/bin/bash

FORCE_CUDA126="${FORCE_CUDA126:-false}"

detect_and_install_pytorch() {
    echo "üîç Detecting GPU hardware..."
    
    # Check for NVIDIA GPU
    if command -v nvidia-smi &> /dev/null && nvidia-smi > /dev/null 2>&1; then
        echo "‚úÖ NVIDIA GPU detected"
        
        # NVIDIA Universal mode (force CUDA 12.6)
        if [ "$FORCE_CUDA126" = "true" ]; then
            echo "üîß Using NVIDIA Universal mode (CUDA 12.6)"
            echo "‚¨áÔ∏è Installing PyTorch 2.9.0 with CUDA 12.6"
            pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 \
                --index-url https://download.pytorch.org/whl/cu126
            return
        fi
        
        # Auto-detect CUDA version based on driver
        if command -v nvidia-smi &> /dev/null; then
            DRIVER_VERSION=$(nvidia-smi --query-gpu=driver_version --format=csv,noheader | head -1)
            echo "üìä NVIDIA Driver version: $DRIVER_VERSION"
            
            # Extract major version
            DRIVER_MAJOR=$(echo "$DRIVER_VERSION" | cut -d. -f1)
            
            if [ "$DRIVER_MAJOR" -ge 535 ]; then  # Driver 535+
                echo "‚¨áÔ∏è Installing PyTorch with CUDA 12.6 (latest)"
                pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 \
                    --index-url https://download.pytorch.org/whl/cu126
            elif [ "$DRIVER_MAJOR" -ge 525 ]; then  # Driver 525-534
                echo "‚¨áÔ∏è Installing PyTorch with CUDA 11.8"
                pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 \
                    --index-url https://download.pytorch.org/whl/cu118
            elif [ "$DRIVER_MAJOR" -ge 470 ]; then  # Driver 470-524
                echo "‚¨áÔ∏è Installing PyTorch with CUDA 11.3"
                pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 \
                    --index-url https://download.pytorch.org/whl/cu113
            else
                echo "‚ö†Ô∏è Old NVIDIA driver, using CUDA 12.6 as fallback"
                pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 \
                    --index-url https://download.pytorch.org/whl/cu126
            fi
        else
            echo "‚¨áÔ∏è Installing PyTorch with CUDA 12.6 (auto-fallback)"
            pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 \
                --index-url https://download.pytorch.org/whl/cu126
        fi
        
    # Check for AMD GPU (ROCm)
    elif [[ -d "/dev/dri" ]] && lspci | grep -i "amd" | grep -i "vga" &> /dev/null; then
        echo "‚úÖ AMD GPU detected (ROCm)"
        echo "‚¨áÔ∏è Installing PyTorch with ROCm 6.4"
        pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 \
            --index-url https://download.pytorch.org/whl/rocm6.4
    
    # Check for Intel GPU
    elif lspci | grep -i "intel" | grep -i "vga" &> /dev/null; then
        echo "‚úÖ Intel GPU detected"
        echo "‚¨áÔ∏è Installing PyTorch with XPU"
        pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 \
            --index-url https://download.pytorch.org/whl/xpu
    
    else
        echo "‚ö†Ô∏è No GPU detected - installing CPU version"
        pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0
    fi
}

detect_and_install_pytorch
EOF

# Copy requirements and setup script
COPY --chmod=755 << 'EOF' /app/setup_comfy.sh
#!/bin/bash

# Set FORCE_CUDA126 from environment variable
export FORCE_CUDA126="${FORCE_CUDA126:-false}"

echo "üîß Configuration:"
echo "   - FORCE_CUDA126: $FORCE_CUDA126"
echo "   - NVIDIA Universal mode: $( [ "$FORCE_CUDA126" = "true" ] && echo "ENABLED" || echo "DISABLED" )"

# Source GPU detection script
source /app/detect_gpu.sh

echo "üì¶ Installing ComfyUI requirements..."
cd /app/comfy-ui

# Download requirements if not present
if [ ! -f "requirements.txt" ]; then
    wget -q https://raw.githubusercontent.com/comfyanonymous/ComfyUI/refs/heads/master/requirements.txt
fi

# Install requirements
pip install -r requirements.txt

# Install comfy-cli
pip install comfy-cli

# Install additional useful packages
pip install \
    pillow \
    numpy \
    scipy \
    pandas \
    matplotlib \
    opencv-python-headless \
    transformers \
    accelerate \
    safetensors \
    websockets \
    aiohttp

echo "‚úÖ ComfyUI setup complete!"
echo "üìä Installed PyTorch version: $(python -c "import torch; print(torch.__version__)")"
if command -v nvidia-smi &> /dev/null; then
    echo "üéÆ CUDA available: $(python -c "import torch; print(torch.cuda.is_available())")"
fi
EOF

# Copy entrypoint script
COPY --chmod=755 << 'EOF' /app/entrypoint.sh
#!/bin/bash

# Export environment variables for GPU detection
export FORCE_CUDA126="${FORCE_CUDA126:-false}"

echo "üöÄ Starting ComfyUI Docker Container"
echo "===================================="
echo "Python version: $(python --version)"
echo "FORCE_CUDA126: $FORCE_CUDA126"
echo "Working directory: $(pwd)"
echo ""

# Initialize ComfyUI if not already done
if [ ! -d "/app/comfy-ui" ] || [ ! "$(ls -A /app/comfy-ui)" ]; then
    echo "üîÑ Initializing ComfyUI for the first time..."
    comfy-cli install /app/comfy-ui
fi

# Run setup script
source /app/setup_comfy.sh

echo ""
echo "‚úÖ Environment ready!"
echo "üåê Web UI accessible at: http://localhost:${HOST_PORT:-8188}"
echo "üí° Use 'docker exec -it ${CONTAINER_NAME:-comfy-cli} /bin/bash' for shell access"
echo ""

# Execute passed command or start comfy-cli
if [ $# -gt 0 ]; then
    echo "‚ö° Executing: $@"
    exec "$@"
else
    echo "‚ö° Starting ComfyUI CLI..."
    exec comfy-cli "$@"
fi
EOF

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1
ENV PIP_NO_CACHE_DIR=1
ENV COMFYUI_PATH=/app/comfy-ui
ENV MODEL_PATH=/app/models
ENV OUTPUT_PATH=/app/output
ENV HOST_PORT=8188
ENV CONTAINER_NAME=comfy-cli
ENV FORCE_CUDA126=false

# Set entrypoint
ENTRYPOINT ["/app/entrypoint.sh"]
CMD []
DOCKERFILE

    # Build the Docker image
    if docker build -t "$IMAGE_NAME" .; then
        echo "‚úÖ Docker image built successfully: $IMAGE_NAME"
        echo "üì¶ Image includes:"
        echo "   ‚Ä¢ Python 3.13"
        echo "   ‚Ä¢ NVIDIA Universal mode support (-nu flag)"
        echo "   ‚Ä¢ Automatic GPU detection"
        echo "   ‚Ä¢ ComfyUI CLI pre-configured"
    else
        echo "‚ùå Failed to build Docker image"
        exit 1
    fi
}

# Build GPU arguments for docker run
build_gpu_args() {
    if [ "$USE_GPU" = true ]; then
        # Check for NVIDIA
        if command -v nvidia-smi &> /dev/null; then
            echo "--gpus all"
        # Check for AMD (simplified detection)
        elif [ -d "/dev/dri" ] && [ -d "/dev/kfd" ]; then
            echo "--device /dev/kfd --device /dev/dri --group-add video"
        # Check for Intel
        elif [ -d "/dev/dri" ]; then
            echo "--device /dev/dri --group-add video"
        else
            echo "‚ö†Ô∏è GPU flag set but no GPU detected, running in CPU mode"
            echo ""
        fi
    else
        echo ""
    fi
}

# Run container
run_container() {
    echo "üöÄ Starting ComfyUI container..."
    
    # Check if image exists
    if ! docker image inspect "$IMAGE_NAME" &> /dev/null; then
        echo "‚ö†Ô∏è Image not found. Building first..."
        build_image
    fi
    
    # Build volume mounts
    VOLUME_MOUNTS="-v ${DATA_VOLUME}:/app"
    
    if [ -n "$HOST_VOLUME" ]; then
        if [ -d "$HOST_VOLUME" ]; then
            VOLUME_MOUNTS="$VOLUME_MOUNTS -v $(realpath "$HOST_VOLUME"):/app/shared"
        else
            echo "‚ö†Ô∏è Host volume directory does not exist: $HOST_VOLUME"
        fi
    fi
    
    # Build GPU arguments
    GPU_ARGS=$(build_gpu_args)
    
    # Set environment variables
    ENV_VARS=""
    if [ "$FORCE_CUDA126" = true ]; then
        ENV_VARS="$ENV_VARS -e FORCE_CUDA126=true"
        echo "üîß NVIDIA Universal mode enabled (CUDA 12.6)"
    fi
    
    # Check if container already exists
    if docker ps -a --format '{{.Names}}' | grep -q "^${CONTAINER_NAME}$"; then
        echo "‚ö†Ô∏è Container '$CONTAINER_NAME' already exists"
        read -p "Stop and remove it? (y/N): " response
        if [[ "$response" =~ ^[Yy]$ ]]; then
            docker stop "$CONTAINER_NAME" 2>/dev/null
            docker rm "$CONTAINER_NAME" 2>/dev/null
        else
            echo "Starting existing container..."
            # Update environment if needed
            if [ "$FORCE_CUDA126" = true ]; then
                echo "‚ö†Ô∏è Note: FORCE_CUDA126 requires container restart to take effect"
            fi
            docker start -ai "$CONTAINER_NAME"
            return
        fi
    fi
    
    echo "üìã Container configuration:"
    echo "   Name: $CONTAINER_NAME"
    echo "   Port: $HOST_PORT:$CONTAINER_PORT"
    echo "   GPU: $( [ "$USE_GPU" = true ] && echo "Enabled" || echo "Disabled" )"
    echo "   NVIDIA Universal: $( [ "$FORCE_CUDA126" = true ] && echo "Enabled (CUDA 12.6)" || echo "Disabled" )"
    echo "   Volume: $DATA_VOLUME"
    [ -n "$HOST_VOLUME" ] && echo "   Host Volume: $HOST_VOLUME"
    echo ""
    
    # Run the container
    docker run -it --rm \
        --name "$CONTAINER_NAME" \
        -p "$HOST_PORT:$CONTAINER_PORT" \
        $GPU_ARGS \
        $VOLUME_MOUNTS \
        -e HOST_UID=$(id -u) \
        -e HOST_GID=$(id -g) \
        -e HOST_PORT="$HOST_PORT" \
        -e CONTAINER_NAME="$CONTAINER_NAME" \
        -e FORCE_CUDA126="$FORCE_CUDA126" \
        -e NVIDIA_VISIBLE_DEVICES=all \
        -e NVIDIA_DRIVER_CAPABILITIES=compute,utility \
        $ENV_VARS \
        "$IMAGE_NAME"
}

# Open shell in container
shell_container() {
    if docker ps --format '{{.Names}}' | grep -q "^${CONTAINER_NAME}$"; then
        echo "üìÇ Opening shell in running container..."
        docker exec -it "$CONTAINER_NAME" /bin/bash
    else
        echo "‚ö†Ô∏è Container '$CONTAINER_NAME' is not running"
        read -p "Start it first? (y/N): " response
        if [[ "$response" =~ ^[Yy]$ ]]; then
            run_container
        fi
    fi
}

# Stop container
stop_container() {
    echo "üõë Stopping container..."
    docker stop "$CONTAINER_NAME" 2>/dev/null && echo "‚úÖ Container stopped"
}

# View logs
view_logs() {
    if docker ps -a --format '{{.Names}}' | grep -q "^${CONTAINER_NAME}$"; then
        echo "üìã Container logs:"
        docker logs "$CONTAINER_NAME"
    else
        echo "‚ö†Ô∏è Container '$CONTAINER_NAME' does not exist"
    fi
}

# Clean up (remove container and image, keep volume)
cleanup() {
    echo "üßπ Cleaning up Docker resources..."
    
    # Stop and remove container
    docker stop "$CONTAINER_NAME" 2>/dev/null
    docker rm "$CONTAINER_NAME" 2>/dev/null
    
    # Remove image
    if docker image inspect "$IMAGE_NAME" &> /dev/null; then
        docker rmi "$IMAGE_NAME" && echo "‚úÖ Image removed"
    fi
    
    echo "üìÅ Data volume '${DATA_VOLUME}' preserved"
}

# Purge everything including volume
purge_all() {
    echo "üî• Purging all Docker resources..."
    
    # Stop and remove container
    docker stop "$CONTAINER_NAME" 2>/dev/null
    docker rm "$CONTAINER_NAME" 2>/dev/null
    
    # Remove image
    docker rmi "$IMAGE_NAME" 2>/dev/null
    
    # Remove volume
    if docker volume inspect "$DATA_VOLUME" &> /dev/null; then
        docker volume rm "$DATA_VOLUME" && echo "‚úÖ Data volume removed"
    fi
    
    # Remove Dockerfile
    [ -f "Dockerfile" ] && rm -f Dockerfile
    
    echo "‚úÖ All resources purged"
}

# Test NVIDIA Universal mode
test_nvidia_universal() {
    echo "üß™ Testing NVIDIA Universal mode..."
    if command -v nvidia-smi &> /dev/null; then
        echo "‚úÖ NVIDIA GPU detected"
        echo "üìä Running test with CUDA 12.6..."
        
        # Create a test container
        docker run --rm --gpus all \
            -e FORCE_CUDA126=true \
            python:3.13-slim \
            bash -c "
            pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 \
                --index-url https://download.pytorch.org/whl/cu126 &&
            python -c \"
import torch
print(f'PyTorch version: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'CUDA version: {torch.version.cuda}')
    print(f'GPU: {torch.cuda.get_device_name(0)}')
\"
            "
    else
        echo "‚ö†Ô∏è No NVIDIA GPU detected"
    fi
}

# Main execution
main() {
    # Show help if requested
    if [[ "$1" == "-h" ]] || [[ "$1" == "--help" ]]; then
        show_help
    fi
    
    # Special test command
    if [[ "$1" == "test-nu" ]]; then
        test_nvidia_universal
        exit 0
    fi
    
    parse_arguments "$@"
    
    case "$COMMAND" in
        run)
            run_container
            ;;
        build)
            build_image
            ;;
        shell)
            shell_container
            ;;
        stop)
            stop_container
            ;;
        logs)
            view_logs
            ;;
        clean)
            cleanup
            ;;
        purge)
            purge_all
            ;;
        help)
            show_help
            ;;
        *)
            echo "‚ùå Unknown command: $COMMAND"
            show_help
            ;;
    esac
}

# Run main function
main "$@"