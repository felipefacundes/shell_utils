#!/usr/bin/env bash
# License: GPLv3
# Credits: Felipe Facundes
# Source: https://github.com/comfyanonymous/ComfyUI
# Tutorial: https://www.youtube.com/watch?v=TLE9NmN_sxw
#           https://youtu.be/xWUddF5oMb0
# Docker version for Python 3.13

# Function to display help menu
show_help() {
    cat << EOF

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                  COMFY-CLI DOCKER SCRIPT (Python 3.13)           â•‘
â•‘                     Containerized Installation                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

DESCRIPTION:
    This script creates and manages a Docker container for ComfyUI CLI
    with Python 3.13, automatic GPU detection, and persistent storage.

USAGE:
    $0 [COMMAND] [OPTIONS]

COMMANDS:
    run         Start ComfyUI CLI in Docker container (default if no command)
    build       Build Docker image with Python 3.13
    shell       Open interactive shell in container
    stop        Stop running container
    logs        View container logs
    clean       Remove container and images (keeps data volume)
    purge       Remove everything including data volume
    help        Show this help message

OPTIONS for 'run' command:
    -g, --gpu          Use GPU acceleration (NVIDIA/AMD)
    -p, --port PORT    Map container port to host (default: 8188)
    -n, --name NAME    Container name (default: comfy-cli)
    -v, --volume DIR   Mount host directory as volume
    -nu, --nvidia-universal  Force CUDA 12.6 for NVIDIA (compatibility mode)

EXAMPLES:
    # Run with GPU support and CUDA 12.6 (compatibility mode)
    $0 run --gpu -nu
    
    # Run with GPU detection (auto)
    $0 run --gpu
    
    # Run with custom port and volume
    $0 run --port 8080 --volume ./models:/app/models
    
    # Build the Docker image
    $0 build
    
    # Open shell in container
    $0 shell
    
    # View logs
    $0 logs

DOCKER IMAGE FEATURES:
    â€¢ Python 3.13 with latest pip
    â€¢ Automatic GPU detection and PyTorch installation
    â€¢ NVIDIA Universal mode (-nu) for CUDA 12.6 compatibility
    â€¢ Persistent data volume: comfy-cli-data
    â€¢ CUDA support for NVIDIA GPUs
    â€¢ ROCm support for AMD GPUs
    â€¢ XPU support for Intel GPUs
    â€¢ CPU fallback mode

PYTORCH INSTALLATION MODES:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Mode             â”‚ PyTorch Version                         â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ NVIDIA (auto)    â”‚ Based on driver version detection       â”‚
    â”‚ NVIDIA (-nu)     â”‚ CUDA 12.6 (torch==2.9.0)                â”‚
    â”‚ AMD              â”‚ ROCm 6.4                                â”‚
    â”‚ Intel            â”‚ XPU                                     â”‚
    â”‚ CPU              â”‚ CPU-only version                        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

VOLUME STRUCTURE:
    â€¢ /app/comfy-ui          - ComfyUI installation
    â€¢ /app/models            - AI models storage
    â€¢ /app/output            - Generated outputs
    â€¢ /app/config            - Configuration files

GPU SUPPORT IN DOCKER:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ GPU Type         â”‚ Docker Runtime / Requirements       â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚ NVIDIA           | --gpus all (requires nvidia-docker2)â”‚
    â”‚ AMD              | --device /dev/kfd --device /dev/dri â”‚
    â”‚ Intel            | --device /dev/dri                   â”‚
    â”‚ CPU              | No special flags needed             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PREREQUISITES:
    â€¢ Docker Engine 20.10+
    â€¢ For NVIDIA: nvidia-docker2 and NVIDIA drivers
    â€¢ For AMD: ROCm Docker support
    â€¢ 10GB+ free disk space

TROUBLESHOOTING:
    â€¢ Check Docker is running: docker info
    â€¢ Test GPU access: docker run --gpus all nvidia/cuda:13.0-base nvidia-smi
    â€¢ Check container status: docker ps -a
    â€¢ View detailed logs: $0 logs
    â€¢ Use -nu flag for NVIDIA compatibility issues

PERSISTENT DATA:
    Data is preserved in Docker volume 'comfy-cli-data'
    To backup: docker run --rm -v comfy-cli-data:/source -v \$(pwd):/backup \\
                alpine tar czf /backup/comfy-backup.tar.gz -C /source .

EOF
    exit 0
}

# Default configuration
CONTAINER_NAME="comfy-cli"
IMAGE_NAME="comfy-cli-python3.13"
DATA_VOLUME="comfy-cli-data"
HOST_PORT="8188"
CONTAINER_PORT="8188"
USE_GPU=false
HOST_VOLUME=""
FORCE_CUDA126=false
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
DOCKER_CONTEXT="${SCRIPT_DIR}/docker-context"

# Parse command line arguments
parse_arguments() {
    COMMAND="${1:-run}"
    
    case "$COMMAND" in
        run|build|shell|stop|logs|clean|purge|help)
            shift
            ;;
        *)
            COMMAND="run"
            ;;
    esac
    
    while [[ $# -gt 0 ]]; do
        case "$1" in
            -g|--gpu)
                USE_GPU=true
                shift
                ;;
            -nu|--nvidia-universal)
                FORCE_CUDA126=true
                shift
                ;;
            -p|--port)
                HOST_PORT="$2"
                shift 2
                ;;
            -n|--name)
                CONTAINER_NAME="$2"
                shift 2
                ;;
            -v|--volume)
                HOST_VOLUME="$2"
                shift 2
                ;;
            -h|--help)
                show_help
                ;;
            *)
                echo "âš ï¸ Unknown option: $1"
                show_help
                ;;
        esac
    done
}

# Create Docker context directory
create_docker_context() {
    echo "ðŸ“ Creating Docker build context..."
    
    # Create directory
    rm -rf "$DOCKER_CONTEXT"
    mkdir -p "$DOCKER_CONTEXT"
    
    # Create Dockerfile
    cat > "$DOCKER_CONTEXT/Dockerfile" << 'EOF'
FROM python:3.13-slim

WORKDIR /app

# Install system dependencies for Debian Trixie/Sid
RUN apt-get update && apt-get install -y \
    wget \
    curl \
    git \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender1 \
    libgl1 \
    libgomp1 \
    ocl-icd-libopencl1 \
    clinfo \
    pciutils \
    libglvnd0 \
    libglx0 \
    libegl1 \
    && rm -rf /var/lib/apt/lists/*

# For NVIDIA GPU support in container
RUN apt-get update && apt-get install -y --no-install-recommends \
    pkg-config \
    libxcursor-dev \
    libxrandr-dev \
    libxinerama-dev \
    libxi-dev \
    && rm -rf /var/lib/apt/lists/*

# Create directories for persistence
RUN mkdir -p /app/comfy-ui /app/models /app/output /app/config

# Copy scripts
COPY scripts/ /app/

# Set permissions
RUN chmod +x /app/*.sh

# Set environment variables
ENV PYTHONUNBUFFERED=1
ENV PYTHONDONTWRITEBYTECODE=1
ENV PIP_NO_CACHE_DIR=1
ENV COMFYUI_PATH=/app/comfy-ui
ENV MODEL_PATH=/app/models
ENV OUTPUT_PATH=/app/output
ENV HOST_PORT=8188
ENV CONTAINER_NAME=comfy-cli
ENV FORCE_CUDA126=false
ENV DEBIAN_FRONTEND=noninteractive

# Set entrypoint
ENTRYPOINT ["/app/entrypoint.sh"]
CMD []
EOF
    
    # Create scripts directory
    mkdir -p "$DOCKER_CONTEXT/scripts"
    
    # Create detect_gpu.sh
    cat > "$DOCKER_CONTEXT/scripts/detect_gpu.sh" << 'SCRIPT_EOF'
#!/bin/bash

FORCE_CUDA126="${FORCE_CUDA126:-false}"

detect_and_install_pytorch() {
    echo "ðŸ” Detecting GPU hardware..."
    
    # Check for NVIDIA GPU
    if command -v nvidia-smi &> /dev/null && nvidia-smi > /dev/null 2>&1; then
        echo "âœ… NVIDIA GPU detected"
        
        # NVIDIA Universal mode (force CUDA 12.6)
        if [ "$FORCE_CUDA126" = "true" ]; then
            echo "ðŸ”§ Using NVIDIA Universal mode (CUDA 12.6)"
            echo "â¬‡ï¸ Installing PyTorch 2.9.0 with CUDA 12.6"
            pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 \
                --index-url https://download.pytorch.org/whl/cu126
            return
        fi
        
        # Auto-detect CUDA version based on driver
        if command -v nvidia-smi &> /dev/null; then
            DRIVER_VERSION=$(nvidia-smi --query-gpu=driver_version --format=csv,noheader | head -1)
            echo "ðŸ“Š NVIDIA Driver version: $DRIVER_VERSION"
            
            # Extract major version
            DRIVER_MAJOR=$(echo "$DRIVER_VERSION" | cut -d. -f1)
            
            if [ "$DRIVER_MAJOR" -ge 535 ]; then  # Driver 535+
                echo "â¬‡ï¸ Installing PyTorch with CUDA 12.6 (latest)"
                pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 \
                    --index-url https://download.pytorch.org/whl/cu126
            elif [ "$DRIVER_MAJOR" -ge 525 ]; then  # Driver 525-534
                echo "â¬‡ï¸ Installing PyTorch with CUDA 11.8"
                pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 \
                    --index-url https://download.pytorch.org/whl/cu118
            elif [ "$DRIVER_MAJOR" -ge 470 ]; then  # Driver 470-524
                echo "â¬‡ï¸ Installing PyTorch with CUDA 11.3"
                pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 \
                    --index-url https://download.pytorch.org/whl/cu113
            else
                echo "âš ï¸ Old NVIDIA driver, using CUDA 12.6 as fallback"
                pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 \
                    --index-url https://download.pytorch.org/whl/cu126
            fi
        else
            echo "â¬‡ï¸ Installing PyTorch with CUDA 12.6 (auto-fallback)"
            pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 \
                --index-url https://download.pytorch.org/whl/cu126
        fi
        
    # Check for AMD GPU (ROCm)
    elif [[ -d "/dev/dri" ]] && lspci | grep -i "amd" | grep -i "vga" &> /dev/null; then
        echo "âœ… AMD GPU detected (ROCm)"
        echo "â¬‡ï¸ Installing PyTorch with ROCm 6.4"
        pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 \
            --index-url https://download.pytorch.org/whl/rocm6.4
    
    # Check for Intel GPU
    elif lspci | grep -i "intel" | grep -i "vga" &> /dev/null; then
        echo "âœ… Intel GPU detected"
        echo "â¬‡ï¸ Installing PyTorch with XPU"
        pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 \
            --index-url https://download.pytorch.org/whl/xpu
    
    else
        echo "âš ï¸ No GPU detected - installing CPU version"
        pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0
    fi
}

detect_and_install_pytorch
SCRIPT_EOF
    
    # Create setup_comfy.sh
    cat > "$DOCKER_CONTEXT/scripts/setup_comfy.sh" << 'SCRIPT_EOF'
#!/bin/bash

# Set FORCE_CUDA126 from environment variable
export FORCE_CUDA126="${FORCE_CUDA126:-false}"

echo "ðŸ”§ Configuration:"
echo "   - FORCE_CUDA126: $FORCE_CUDA126"
echo "   - NVIDIA Universal mode: $( [ "$FORCE_CUDA126" = "true" ] && echo "ENABLED" || echo "DISABLED" )"

# Source GPU detection script
source /app/detect_gpu.sh

echo "ðŸ“¦ Installing ComfyUI requirements..."
cd /app/comfy-ui

# Download requirements if not present
if [ ! -f "requirements.txt" ]; then
    echo "â¬‡ï¸ Downloading ComfyUI requirements..."
    wget -q https://raw.githubusercontent.com/comfyanonymous/ComfyUI/refs/heads/master/requirements.txt || \
    curl -s -o requirements.txt https://raw.githubusercontent.com/comfyanonymous/ComfyUI/refs/heads/master/requirements.txt
fi

# Install requirements
if [ -f "requirements.txt" ]; then
    echo "ðŸ“¥ Installing from requirements.txt..."
    pip install -r requirements.txt
else
    echo "âš ï¸ requirements.txt not found, installing basic packages..."
fi

# Install comfy-cli
echo "ðŸ“¦ Installing comfy-cli..."
pip install comfy-cli

# Install additional useful packages
echo "ðŸ“¦ Installing additional packages..."
pip install --upgrade pip
pip install \
    pillow \
    numpy \
    scipy \
    pandas \
    matplotlib \
    opencv-python-headless \
    transformers \
    accelerate \
    safetensors \
    websockets \
    aiohttp \
    torchsde \
    kornia \
    rich

echo "âœ… ComfyUI setup complete!"
echo "ðŸ“Š Installed PyTorch version: $(python -c "import torch; print(torch.__version__)")"
if command -v nvidia-smi &> /dev/null; then
    echo "ðŸŽ® Checking CUDA availability..."
    python -c "
import torch
print(f'CUDA available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'CUDA version: {torch.version.cuda}')
    print(f'GPU: {torch.cuda.get_device_name(0)}')
    print(f'GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB')
"
fi
SCRIPT_EOF
    
    # Create entrypoint.sh
    cat > "$DOCKER_CONTEXT/scripts/entrypoint.sh" << 'SCRIPT_EOF'
#!/bin/bash

# Export environment variables for GPU detection
export FORCE_CUDA126="${FORCE_CUDA126:-false}"

echo "ðŸš€ Starting ComfyUI Docker Container"
echo "===================================="
echo "Python version: $(python --version)"
echo "FORCE_CUDA126: $FORCE_CUDA126"
echo "Working directory: $(pwd)"
echo ""

# Check for GPU
echo "ðŸ” Checking system configuration..."
if command -v nvidia-smi &> /dev/null; then
    echo "âœ… NVIDIA GPU detected in container"
    nvidia-smi --query-gpu=name,driver_version --format=csv
elif [[ -d "/dev/dri" ]]; then
    echo "âœ… AMD/Intel GPU detected (OpenGL available)"
else
    echo "â„¹ï¸  Running in CPU mode"
fi
echo ""

# Initialize ComfyUI if not already done
if [ ! -d "/app/comfy-ui" ] || [ ! "$(ls -A /app/comfy-ui)" ]; then
    echo "ðŸ”„ Initializing ComfyUI for the first time..."
    comfy-cli install /app/comfy-ui --force
else
    echo "âœ… ComfyUI already installed"
fi

# Run setup script
source /app/setup_comfy.sh

echo ""
echo "âœ… Environment ready!"
echo "ðŸŒ Web UI accessible at: http://localhost:${HOST_PORT:-8188}"
echo "ðŸ’¡ Use 'docker exec -it ${CONTAINER_NAME:-comfy-cli} /bin/bash' for shell access"
echo ""

# Execute passed command or start comfy-cli
if [ $# -gt 0 ]; then
    echo "âš¡ Executing: $@"
    exec "$@"
else
    echo "âš¡ Starting ComfyUI CLI..."
    exec comfy-cli "$@"
fi
SCRIPT_EOF
    
    echo "âœ… Docker context created at: $DOCKER_CONTEXT"
}

# Build Docker image with Python 3.13
build_image() {
    echo "ðŸš€ Building Docker image with Python 3.13..."
    
    # Create Docker context
    create_docker_context
    
    # Build the Docker image
    echo "ðŸ”¨ Building image '$IMAGE_NAME'..."
    if docker build --progress=plain -t "$IMAGE_NAME" "$DOCKER_CONTEXT"; then
        echo "âœ… Docker image built successfully: $IMAGE_NAME"
        echo "ðŸ“¦ Image includes:"
        echo "   â€¢ Python 3.13"
        echo "   â€¢ NVIDIA Universal mode support (-nu flag)"
        echo "   â€¢ Automatic GPU detection"
        echo "   â€¢ ComfyUI CLI pre-configured"
        
        # Show image details
        echo ""
        echo "ðŸ“Š Image details:"
        docker images "$IMAGE_NAME"
    else
        echo "âŒ Failed to build Docker image"
        exit 1
    fi
}

# Build GPU arguments for docker run
build_gpu_args() {
    if [ "$USE_GPU" = true ]; then
        # Check for NVIDIA
        if command -v nvidia-smi &> /dev/null; then
            echo "--gpus all"
        # Check for AMD (simplified detection)
        elif [ -d "/dev/dri" ] && [ -d "/dev/kfd" ]; then
            echo "--device /dev/kfd --device /dev/dri --group-add video"
        # Check for Intel
        elif [ -d "/dev/dri" ]; then
            echo "--device /dev/dri --group-add video"
        else
            echo "âš ï¸ GPU flag set but no GPU detected, running in CPU mode"
            echo ""
        fi
    else
        echo ""
    fi
}

# Run container
run_container() {
    echo "ðŸš€ Starting ComfyUI container..."
    
    # Check if image exists
    if ! docker image inspect "$IMAGE_NAME" &> /dev/null; then
        echo "âš ï¸ Image not found. Building first..."
        build_image
    fi
    
    # Build volume mounts
    VOLUME_MOUNTS="-v ${DATA_VOLUME}:/app"
    
    if [ -n "$HOST_VOLUME" ]; then
        if [ -d "$HOST_VOLUME" ]; then
            VOLUME_MOUNTS="$VOLUME_MOUNTS -v $(realpath "$HOST_VOLUME"):/app/shared"
            echo "ðŸ“‚ Mounting host directory: $HOST_VOLUME â†’ /app/shared"
        else
            echo "âš ï¸ Host volume directory does not exist: $HOST_VOLUME"
        fi
    fi
    
    # Build GPU arguments
    GPU_ARGS=$(build_gpu_args)
    
    # Set environment variables
    ENV_VARS=""
    if [ "$FORCE_CUDA126" = true ]; then
        ENV_VARS="$ENV_VARS -e FORCE_CUDA126=true"
        echo "ðŸ”§ NVIDIA Universal mode enabled (CUDA 12.6)"
    fi
    
    # Check if container already exists
    if docker ps -a --format '{{.Names}}' | grep -q "^${CONTAINER_NAME}$"; then
        echo "âš ï¸ Container '$CONTAINER_NAME' already exists"
        read -p "Stop and remove it? (y/N): " response
        if [[ "$response" =~ ^[Yy]$ ]]; then
            docker stop "$CONTAINER_NAME" 2>/dev/null
            docker rm "$CONTAINER_NAME" 2>/dev/null
            echo "âœ… Container removed"
        else
            echo "Starting existing container..."
            # Update environment if needed
            if [ "$FORCE_CUDA126" = true ]; then
                echo "âš ï¸ Note: FORCE_CUDA126 requires container restart to take effect"
            fi
            docker start -ai "$CONTAINER_NAME"
            return
        fi
    fi
    
    echo "ðŸ“‹ Container configuration:"
    echo "   Name: $CONTAINER_NAME"
    echo "   Port: $HOST_PORT:$CONTAINER_PORT"
    echo "   GPU: $( [ "$USE_GPU" = true ] && echo "Enabled" || echo "Disabled" )"
    echo "   NVIDIA Universal: $( [ "$FORCE_CUDA126" = true ] && echo "Enabled (CUDA 12.6)" || echo "Disabled" )"
    echo "   Volume: $DATA_VOLUME"
    [ -n "$HOST_VOLUME" ] && echo "   Host Volume: $HOST_VOLUME"
    echo ""
    
    # Create data volume if it doesn't exist
    if ! docker volume inspect "$DATA_VOLUME" &> /dev/null; then
        echo "ðŸ“ Creating data volume: $DATA_VOLUME"
        docker volume create "$DATA_VOLUME"
    fi
    
    # Run the container
    echo "ðŸ³ Starting Docker container..."
    docker run -it --rm \
        --name "$CONTAINER_NAME" \
        -p "$HOST_PORT:$CONTAINER_PORT" \
        $GPU_ARGS \
        $VOLUME_MOUNTS \
        -e HOST_UID=$(id -u) \
        -e HOST_GID=$(id -g) \
        -e HOST_PORT="$HOST_PORT" \
        -e CONTAINER_NAME="$CONTAINER_NAME" \
        -e FORCE_CUDA126="$FORCE_CUDA126" \
        -e NVIDIA_VISIBLE_DEVICES=all \
        -e NVIDIA_DRIVER_CAPABILITIES=compute,utility,graphics \
        $ENV_VARS \
        "$IMAGE_NAME"
}

# Open shell in container
shell_container() {
    if docker ps --format '{{.Names}}' | grep -q "^${CONTAINER_NAME}$"; then
        echo "ðŸ“‚ Opening shell in running container..."
        docker exec -it "$CONTAINER_NAME" /bin/bash
    else
        echo "âš ï¸ Container '$CONTAINER_NAME' is not running"
        read -p "Start it first? (y/N): " response
        if [[ "$response" =~ ^[Yy]$ ]]; then
            run_container
        fi
    fi
}

# Stop container
stop_container() {
    echo "ðŸ›‘ Stopping container..."
    if docker stop "$CONTAINER_NAME" 2>/dev/null; then
        echo "âœ… Container stopped"
    else
        echo "âš ï¸ Container not running or doesn't exist"
    fi
}

# View logs
view_logs() {
    if docker ps -a --format '{{.Names}}' | grep -q "^${CONTAINER_NAME}$"; then
        echo "ðŸ“‹ Container logs:"
        docker logs "$CONTAINER_NAME"
    else
        echo "âš ï¸ Container '$CONTAINER_NAME' does not exist"
    fi
}

# Clean up (remove container and image, keep volume)
cleanup() {
    echo "ðŸ§¹ Cleaning up Docker resources..."
    
    # Stop and remove container
    docker stop "$CONTAINER_NAME" 2>/dev/null
    docker rm "$CONTAINER_NAME" 2>/dev/null 2>&1
    
    # Remove image
    if docker image inspect "$IMAGE_NAME" &> /dev/null; then
        docker rmi "$IMAGE_NAME" && echo "âœ… Image removed"
    else
        echo "â„¹ï¸  Image not found"
    fi
    
    # Clean up Docker context
    rm -rf "$DOCKER_CONTEXT" 2>&1
    
    echo "ðŸ“ Data volume '${DATA_VOLUME}' preserved"
}

# Purge everything including volume
purge_all() {
    echo "ðŸ”¥ Purging all Docker resources..."
    
    # Stop and remove container
    docker stop "$CONTAINER_NAME" 2>/dev/null
    docker rm "$CONTAINER_NAME" 2>/dev/null 2>&1
    
    # Remove image
    docker rmi "$IMAGE_NAME" 2>/dev/null 2>&1
    
    # Remove volume
    if docker volume inspect "$DATA_VOLUME" &> /dev/null; then
        docker volume rm "$DATA_VOLUME" && echo "âœ… Data volume removed"
    else
        echo "â„¹ï¸  Data volume not found"
    fi
    
    # Clean up Docker context
    rm -rf "$DOCKER_CONTEXT" 2>&1
    
    echo "âœ… All resources purged"
}

# Test NVIDIA Universal mode
test_nvidia_universal() {
    echo "ðŸ§ª Testing NVIDIA Universal mode..."
    if command -v nvidia-smi &> /dev/null; then
        echo "âœ… NVIDIA GPU detected"
        echo "ðŸ“Š Running test with CUDA 12.6..."
        
        # Create a test container
        docker run --rm --gpus all \
            -e FORCE_CUDA126=true \
            python:3.13-slim \
            bash -c "
            pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 \
                --index-url https://download.pytorch.org/whl/cu126 &&
            python -c \"
import torch
print(f'PyTorch version: {torch.__version__}')
print(f'CUDA available: {torch.cuda.is_available()}')
if torch.cuda.is_available():
    print(f'CUDA version: {torch.version.cuda}')
    print(f'GPU: {torch.cuda.get_device_name(0)}')
\"
            "
    else
        echo "âš ï¸ No NVIDIA GPU detected"
    fi
}

# Quick build with minimal dependencies (for testing)
quick_build() {
    echo "âš¡ Quick build with minimal dependencies..."
    
    # Create minimal Dockerfile
    cat > /tmp/Dockerfile.comfy << 'EOF'
FROM python:3.13-slim

WORKDIR /app

# Install minimal dependencies
RUN apt-get update && apt-get install -y \
    wget \
    curl \
    git \
    && rm -rf /var/lib/apt/lists/*

# Create directories
RUN mkdir -p /app/comfy-ui

# Set environment
ENV PYTHONUNBUFFERED=1
ENV PIP_NO_CACHE_DIR=1

# Install PyTorch with CUDA 12.6 (universal)
RUN pip install torch==2.9.0 torchvision==0.24.0 torchaudio==2.9.0 \
    --index-url https://download.pytorch.org/whl/cu126

# Install comfy-cli
RUN pip install comfy-cli

CMD ["comfy-cli", "--help"]
EOF
    
    if docker build -t "comfy-cli-quick" -f /tmp/Dockerfile.comfy .; then
        echo "âœ… Quick build successful!"
        echo "Test with: docker run --rm --gpus all comfy-cli-quick"
    fi
}

# Main execution
main() {
    # Show help if requested
    if [[ "$1" == "-h" ]] || [[ "$1" == "--help" ]]; then
        show_help
    fi
    
    # Special test command
    if [[ "$1" == "test-nu" ]]; then
        test_nvidia_universal
        exit 0
    fi
    
    # Quick build command
    if [[ "$1" == "quick-build" ]]; then
        quick_build
        exit 0
    fi
    
    parse_arguments "$@"
    
    case "$COMMAND" in
        run)
            run_container
            ;;
        build)
            build_image
            ;;
        shell)
            shell_container
            ;;
        stop)
            stop_container
            ;;
        logs)
            view_logs
            ;;
        clean)
            cleanup
            ;;
        purge)
            purge_all
            ;;
        help)
            show_help
            ;;
        *)
            echo "âŒ Unknown command: $COMMAND"
            show_help
            ;;
    esac
}

# Run main function
main "$@"